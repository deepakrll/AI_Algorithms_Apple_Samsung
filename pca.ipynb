#5. PCA
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load dataset
file_path = '/content/drive/My Drive/Colab Notebooks/iphone_samsung_sales_with_models.csv'
df = pd.read_csv(file_path)

# Display first few rows of the dataset
print("First few rows of the dataset:")
print(df.head())

# Encode categorical variables (State, Brand, Model) into numerical format
le = LabelEncoder()
df['State'] = le.fit_transform(df['State'])
df['Brand'] = le.fit_transform(df['Brand'])
df['Model'] = le.fit_transform(df['Model'])

# Select features for PCA (excluding Year, because it is temporal)
X = df[['State', 'Brand', 'Model', 'Sales Units', 'Revenue (INR)']]

# Standardize the data before applying PCA (important step for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA to reduce dimensions (let's reduce to 2 components for visualization)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create a new DataFrame with PCA results
df_pca = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])

# Add PCA results to the original DataFrame for comparison
df_final = pd.concat([df, df_pca], axis=1)

# Print explained variance to understand how much information each component holds
print("\nExplained variance by each principal component:")
print(pca.explained_variance_ratio_)

# Visualize the results of PCA (scatter plot)
plt.figure(figsize=(10, 6))
plt.scatter(df_pca['Principal Component 1'], df_pca['Principal Component 2'], c=df['Brand'], cmap='viridis')
plt.colorbar(label='Brand')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of iPhone and Samsung Sales Data')
plt.show()
